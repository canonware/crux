--------------------------------------------------------------------------------
17 September 2004
--------------------------------------------------------------------------------

With the conversion of Crux's NJ implementation to heuristic clustering, the
performance has increased by roughly two orders of magnitude.  However, this
performance gain is not guaranteed; the algorithm is still O(n^3) in the worst
case.  The question of how often the worst case shows up is therefore of
interest.  It turns out that worst case performance requires a specially shaped
tree, and the taxa have to be listed in the matrix in just the right order.
As a result of the second factor (ordering), there turns out to be a simple way
to make sure that the worst case never shows up in practice.  Since the number
of possible orderings grows factorially with the number of taxa, we can
randomize the starting order of the matrix, thus assuring that hitting the worst
case performance happens 1/n! of the time.

What about nearly worst case performance?  This happens more often, but again,
it happens very seldom, relative to the total number of possible orderings.

Anyway, I don't know that it's worthwhile implementing randomization, but it
probably is worth mentioning in the paper about this algorithm.

--------------------------------------------------------------------------------
26 September 2004
--------------------------------------------------------------------------------

It may make sense to argue that Crux's NJ implementation is better for tree
sampling, since it potentially generates more trees, without breaking the
constraints of the NJ algorithm.  Exact ties seldom show up when using IEEE
floating point math, so unless an approximate equality test is used, the
standard NJ algorithm is especially limiting with regard to tree sampling.

--------------------------------------------------------------------------------
9 October 2004
--------------------------------------------------------------------------------

I implemented uniform sampling of tied transformed distances for random NJ, but
it occurs to me that there is still one source of bias for random NJ: Random row
selection is uniform, but it should be proportional to the number of minimum
transformed distances on each row.  In practice, this is a small bias, but
without somehow addressing this, I cannot claim completely unbiased random NJ.

--------------------------------------------------------------------------------
21 November 2004
--------------------------------------------------------------------------------

The randomized RNJ algorithm needs to iterate vertically over portions of the
distance matrix.  Consider that it would be possible to store the upper and
lower triangles of a symmetric matrix (2X as much memory), and all iteration
could be done linearly in memory.  Matrix reductions could still be done with
the fast algorithm for both triangles.  This would increase cache load for
random reads, but linear reads would probably be quite a bit faster in all
cases.

--------------------------------------------------------------------------------
26 November 2004
--------------------------------------------------------------------------------

It occurs to me that NJ may be capable of making the same type of mistake as RNJ
makes (join nodes that are very close to each other, but that are separated by
an internal edge.  Try to come up with an example that allows this to happen.

(The NJ proofs say that this can't happen.  Gain an understanding of why this is
the case.)

--------------------------------------------------------------------------------
30 December 2004
--------------------------------------------------------------------------------

An alternative strategy for finding pairs of taxa to join is to try to make use
of failed choices by chasing down a taxon for which a minimal tdist was found.
For example, if A and B are chosen, but C is too close to B, then try B and C,
and if that fails, try C and whatever makes it fail, etc.  I think it may be
possible to get stuck in a cycle, so some method of avoiding that would be
necessary (give up after a certain number of attempts, keep a log, or some other
method).

--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
